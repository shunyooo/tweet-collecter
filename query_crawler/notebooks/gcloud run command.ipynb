{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def call(cmd):\n",
    "    subprocess.call(cmd , shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "path = os.path.join(os.path.abspath(os.curdir), '../')\n",
    "sys.path.append(path)\n",
    "path = os.path.join(os.path.abspath(os.curdir), '../../')\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read comic list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save():\n",
    "    global comic_list, rubi_list, num_split_list\n",
    "    _save = lambda obj, path: json.dump(obj, open(path, 'w'))\n",
    "    _save(comic_list, 'comic_list.json')\n",
    "    _save(rubi_list, 'rubi_list.json')\n",
    "    _save(num_split_list, 'num_split_list.json')\n",
    "    print('saved')\n",
    "    \n",
    "def load():\n",
    "    global comic_list, rubi_list, num_split_list\n",
    "    _load = lambda path: json.load(open(path, 'r'))\n",
    "    comic_list = _load('comic_list.json')\n",
    "    rubi_list = _load('rubi_list.json')\n",
    "    num_split_list = _load('num_split_list.json')\n",
    "    print('loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "load()\n",
    "comic_rubi_numsplit_list = list(zip(comic_list, rubi_list, num_split_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ワンピース', 'onepiece', 100),\n",
       " ('キングダム', 'kingdom', 100),\n",
       " ('名探偵コナン', 'konan', 100),\n",
       " ('ジョジョ', 'jojo', 100),\n",
       " ('キン肉マン', 'kiniku-man', 100),\n",
       " ('ナルト', 'naruto', 100),\n",
       " ('ドラゴンボール', 'doragon-ball', 100),\n",
       " ('キャプテン翼', 'kyapten-tsubasa', 50),\n",
       " ('ガッシュ', 'gashu', 50),\n",
       " ('銀魂', 'gintama', 100),\n",
       " ('僕のヒーローアカデミア', 'bokuno-hero-academia', 30),\n",
       " ('亜人', 'ajin', 30),\n",
       " ('進撃の巨人', 'singeki-no-kyojin', 30),\n",
       " ('テラフォーマー', 'terra-former', 30),\n",
       " ('ワンパンマン', 'one-punch-man', 30),\n",
       " ('約束のネバーランド', 'yakusoku-no-neverland', 30),\n",
       " ('Dr.STONE', 'dr-stone', 30),\n",
       " ('チェンソーマン', 'tyenso-man', 10),\n",
       " ('かぐや様', 'kaguyasama', 20),\n",
       " ('ダンジョン飯', 'danjon-meshi', 20),\n",
       " ('ワンナウツ', 'oneouts', 10),\n",
       " ('BANANA FISH', 'banana-fish', 20),\n",
       " ('AKIRA', 'akira', 100),\n",
       " ('君に届け', 'kimini-todoke', 5),\n",
       " ('狼陛下の花嫁', 'okami-heika', 5),\n",
       " ('フィンランドサガ', 'finland-saga', 1),\n",
       " ('雀荘のサエコさん', 'saeko', 1),\n",
       " ('ジモトがジャパン', 'jimoto', 1),\n",
       " ('SILENT NIGHT 翔', 'silent-night-syo', 1),\n",
       " ('私立ポセイドン学園高等部', 'poseidon', 1),\n",
       " ('チャゲチャ', 'tyagetya', 2),\n",
       " ('大泥棒ポルタ', 'poruta', 3)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comic_rubi_numsplit_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read comic list from db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read config → ../config/config.ini\n"
     ]
    }
   ],
   "source": [
    "from comic_crawler.database import init_database, refresh_session, engine, Base, Comic, ComicVolume, init_database\n",
    "session = refresh_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comic_vols_q = session.query(ComicVolume).filter(ComicVolume.published_on.between('2019-06-01', '2019-08-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-01 18:27:01,832 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'sql_mode'\n",
      "2019-08-01 18:27:01,832 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-01 18:27:01,840 INFO sqlalchemy.engine.base.Engine SELECT DATABASE()\n",
      "2019-08-01 18:27:01,841 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-01 18:27:01,846 INFO sqlalchemy.engine.base.Engine show collation where `Charset` = 'utf8mb4' and `Collation` = 'utf8mb4_bin'\n",
      "2019-08-01 18:27:01,847 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-01 18:27:01,851 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS CHAR(60)) AS anon_1\n",
      "2019-08-01 18:27:01,851 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-01 18:27:01,854 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS CHAR(60)) AS anon_1\n",
      "2019-08-01 18:27:01,855 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-01 18:27:01,857 INFO sqlalchemy.engine.base.Engine SELECT CAST('test collated returns' AS CHAR CHARACTER SET utf8mb4) COLLATE utf8mb4_bin AS anon_1\n",
      "2019-08-01 18:27:01,858 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-08-01 18:27:01,864 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)\n",
      "2019-08-01 18:27:01,865 INFO sqlalchemy.engine.base.Engine SELECT comic_volumes.id AS comic_volumes_id, comic_volumes.volume AS comic_volumes_volume, comic_volumes.published_on AS comic_volumes_published_on, comic_volumes.thumbnail_url AS comic_volumes_thumbnail_url, comic_volumes.summary AS comic_volumes_summary, comic_volumes.comic_id AS comic_volumes_comic_id, comic_volumes.created_at AS comic_volumes_created_at, comic_volumes.updated_at AS comic_volumes_updated_at \n",
      "FROM comic_volumes \n",
      "WHERE comic_volumes.published_on BETWEEN %s AND %s\n",
      "2019-08-01 18:27:01,866 INFO sqlalchemy.engine.base.Engine ('2019-06-01', '2019-08-01')\n"
     ]
    }
   ],
   "source": [
    "volume_list = comic_vols_q.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(volume_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = volume_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comic_list = [session.query(Comic).filter(Comic.id == vol.comic_id).first() for vol in volume_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(instance):\n",
    "    return {k:v for k,v in instance.__dict__.items() if '_' != k[0]}\n",
    "\n",
    "def to_dictlist(instance_list):\n",
    "    return [to_dict(instance) for instance in instance_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comic = pd.DataFrame(to_dictlist(comic_list)).rename(columns={'id':'comic_id'})\n",
    "df_volume = pd.DataFrame(to_dictlist(volume_list)).rename(columns={'id':'volume_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1568, 8), (1568, 5))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_volume.shape, df_comic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comic_volume = pd.merge(df_comic, df_volume, on='comic_id', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comic_volume = df_comic_volume.drop_duplicates(['comic_id', 'volume_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_comic_volume.sort_values('published_on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "po = df_comic_volume.published_on\n",
    "df_comic_volume_7 = df_comic_volume[('2019-06-30' < po)].sort_values('published_on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_roma(t):\n",
    "    t = utils.to_roma(t)# ローマ字に\n",
    "    t = re.sub(r'[^a-z]', '-', t.lower()).strip() # a-z以外を-に\n",
    "    t = re.sub(r'[-]+', '-', t) # -の連続をまとめる\n",
    "    t = re.sub(r'^-|-$', '', t) # 行頭, 行末の-削除\n",
    "    return t\n",
    "\n",
    "def extract_roma(roma, n):\n",
    "    t = roma[:n]\n",
    "    t = re.sub(r'^-|-$', '', t) # 行頭, 行末の-削除\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comic_volume_7['roma'] = df_comic_volume_7.title.apply(lambda title: parse_roma(title))\n",
    "df_comic_volume_7['roma_10'] = df_comic_volume_7.roma.apply(lambda roma: extract_roma(roma, 10))\n",
    "df_comic_volume_7['instance_name'] = [f'{row.roma_10}_{row.comic_id}' for i, row in df_comic_volume_7.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'shinsho　orureriannokishihime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shinsho\\u3000orureriannokishihime'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.strip(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define num of instance by comic\n",
    "漫画ごとのインスタンス数を決める。  \n",
    "2日で10分ぐらい。1月で2.6時間,   \n",
    "本当はここも自動化したい。タスクをDBに入れて、それを消化していく形にしたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from twitterscraper.query import linspace\n",
    "import datetime as dt\n",
    "def split_date(begindate, enddate, num_split):\n",
    "    num_days = (enddate - begindate).days\n",
    "    dateranges = [begindate + dt.timedelta(days=elem) for elem in linspace(0, num_days, num_split+1)]\n",
    "    return dateranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vm_name(basename, dateranges, query=None):\n",
    "    res_list = [{\n",
    "        'name': f'{basename}-{since}-{until}',\n",
    "        'since': since,\n",
    "        'until': until,\n",
    "    } for since, until in zip(dateranges[:-1], dateranges[1:])]\n",
    "    if query is not None:\n",
    "        for res in res_list:\n",
    "            res['query'] = query\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "begindate = dt.date(2006, 3, 21)\n",
    "enddate = dt.date(2019, 6, 12)\n",
    "num_split = 10\n",
    "dateranges = split_date(begindate, enddate, num_split=num_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comic_daterange_list = create_vm_name('onepiece', dateranges)\n",
    "comic_daterange_name_list = [c['name'] for c in comic_daterange_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_query_instance_list = create_vm_name('onepiece', dateranges)\n",
    "for d in crawl_query_instance_list:\n",
    "    d['query'] = 'ワンピース'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ワンピース', 'onepiece', 100),\n",
       " ('キングダム', 'kingdom', 100),\n",
       " ('名探偵コナン', 'konan', 100),\n",
       " ('ジョジョ', 'jojo', 100),\n",
       " ('キン肉マン', 'kiniku-man', 100),\n",
       " ('ナルト', 'naruto', 100),\n",
       " ('ドラゴンボール', 'doragon-ball', 100),\n",
       " ('キャプテン翼', 'kyapten-tsubasa', 50),\n",
       " ('ガッシュ', 'gashu', 50),\n",
       " ('銀魂', 'gintama', 100),\n",
       " ('僕のヒーローアカデミア', 'bokuno-hero-academia', 30),\n",
       " ('亜人', 'ajin', 30),\n",
       " ('進撃の巨人', 'singeki-no-kyojin', 30),\n",
       " ('テラフォーマー', 'terra-former', 30),\n",
       " ('ワンパンマン', 'one-punch-man', 30),\n",
       " ('約束のネバーランド', 'yakusoku-no-neverland', 30),\n",
       " ('Dr.STONE', 'dr-stone', 30),\n",
       " ('チェンソーマン', 'tyenso-man', 10),\n",
       " ('かぐや様', 'kaguyasama', 20),\n",
       " ('ダンジョン飯', 'danjon-meshi', 20),\n",
       " ('ワンナウツ', 'oneouts', 10),\n",
       " ('BANANA FISH', 'banana-fish', 20),\n",
       " ('AKIRA', 'akira', 100),\n",
       " ('君に届け', 'kimini-todoke', 5),\n",
       " ('狼陛下の花嫁', 'okami-heika', 5),\n",
       " ('フィンランドサガ', 'finland-saga', 1),\n",
       " ('雀荘のサエコさん', 'saeko', 1),\n",
       " ('ジモトがジャパン', 'jimoto', 1),\n",
       " ('SILENT NIGHT 翔', 'silent-night-syo', 1),\n",
       " ('私立ポセイドン学園高等部', 'poseidon', 1),\n",
       " ('チャゲチャ', 'tyagetya', 2),\n",
       " ('大泥棒ポルタ', 'poruta', 3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comic_rubi_numsplit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('フィンランドサガ', 'finland-saga', 1),\n",
       " ('雀荘のサエコさん', 'saeko', 1),\n",
       " ('ジモトがジャパン', 'jimoto', 1),\n",
       " ('SILENT NIGHT 翔', 'silent-night-syo', 1),\n",
       " ('私立ポセイドン学園高等部', 'poseidon', 1),\n",
       " ('チャゲチャ', 'tyagetya', 2),\n",
       " ('大泥棒ポルタ', 'poruta', 3),\n",
       " ('君に届け', 'kimini-todoke', 5),\n",
       " ('狼陛下の花嫁', 'okami-heika', 5),\n",
       " ('チェンソーマン', 'tyenso-man', 10),\n",
       " ('ワンナウツ', 'oneouts', 10),\n",
       " ('かぐや様', 'kaguyasama', 20),\n",
       " ('ダンジョン飯', 'danjon-meshi', 20),\n",
       " ('BANANA FISH', 'banana-fish', 20),\n",
       " ('僕のヒーローアカデミア', 'bokuno-hero-academia', 30),\n",
       " ('亜人', 'ajin', 30),\n",
       " ('進撃の巨人', 'singeki-no-kyojin', 30),\n",
       " ('テラフォーマー', 'terra-former', 30),\n",
       " ('ワンパンマン', 'one-punch-man', 30),\n",
       " ('約束のネバーランド', 'yakusoku-no-neverland', 30),\n",
       " ('Dr.STONE', 'dr-stone', 30),\n",
       " ('キャプテン翼', 'kyapten-tsubasa', 50),\n",
       " ('ガッシュ', 'gashu', 50),\n",
       " ('ワンピース', 'onepiece', 100),\n",
       " ('キングダム', 'kingdom', 100),\n",
       " ('名探偵コナン', 'konan', 100),\n",
       " ('ジョジョ', 'jojo', 100),\n",
       " ('キン肉マン', 'kiniku-man', 100),\n",
       " ('ナルト', 'naruto', 100),\n",
       " ('ドラゴンボール', 'doragon-ball', 100),\n",
       " ('銀魂', 'gintama', 100),\n",
       " ('AKIRA', 'akira', 100)]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comic_rubi_numsplit_list.sort(key=lambda x: x[2])\n",
    "comic_rubi_numsplit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "begindate = dt.date(2006, 3, 21)\n",
    "enddate = dt.date.today()\n",
    "crawl_query_instance_list = []\n",
    "for comic, rubi, num_split in comic_rubi_numsplit_list:\n",
    "    dateranges = split_date(begindate, enddate, num_split=num_split)\n",
    "    crawl_query_instance_list += create_vm_name(rubi, dateranges, comic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crawl_query_instance_list[70:70+51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1310"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crawl_query_instance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "_crawl_query_instance_list = crawl_query_instance_list[70:70+51]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_create_command(vm_index_list, image_name='arb-tweet-crawler-base-3', machine_type='g1-small'):\n",
    "    vm_name_list = [f'arb-tweet-crawler-{vm_index}' for vm_index in vm_index_list]\n",
    "    vm_names_text = ' '.join(vm_name_list)\n",
    "    command = f'gcloud compute  --project \"machinelearning-219614\"  instances create {vm_names_text} --machine-type \"{machine_type}\" --zone \"us-central1-a\" --image {image_name} --image-project \"machinelearning-219614\"'\n",
    "    return command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "comic_daterange_name_list = [comic['name'] for comic in _crawl_query_instance_list]\n",
    "create_command = make_create_command(comic_daterange_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['danjon-meshi-2012-11-04-2013-07-04',\n",
       " 'danjon-meshi-2013-07-04-2014-03-03',\n",
       " 'danjon-meshi-2014-03-03-2014-10-31',\n",
       " 'danjon-meshi-2014-10-31-2015-06-30',\n",
       " 'danjon-meshi-2015-06-30-2016-02-27',\n",
       " 'danjon-meshi-2016-02-27-2016-10-26',\n",
       " 'danjon-meshi-2016-10-26-2017-06-25',\n",
       " 'danjon-meshi-2017-06-25-2018-02-22',\n",
       " 'danjon-meshi-2018-02-22-2018-10-22',\n",
       " 'danjon-meshi-2018-10-22-2019-06-22',\n",
       " 'banana-fish-2006-03-21-2006-11-18',\n",
       " 'banana-fish-2006-11-18-2007-07-18',\n",
       " 'banana-fish-2007-07-18-2008-03-16',\n",
       " 'banana-fish-2008-03-16-2008-11-13',\n",
       " 'banana-fish-2008-11-13-2009-07-13',\n",
       " 'banana-fish-2009-07-13-2010-03-12',\n",
       " 'banana-fish-2010-03-12-2010-11-09',\n",
       " 'banana-fish-2010-11-09-2011-07-09',\n",
       " 'banana-fish-2011-07-09-2012-03-07',\n",
       " 'banana-fish-2012-03-07-2012-11-04',\n",
       " 'banana-fish-2012-11-04-2013-07-04',\n",
       " 'banana-fish-2013-07-04-2014-03-03',\n",
       " 'banana-fish-2014-03-03-2014-10-31',\n",
       " 'banana-fish-2014-10-31-2015-06-30',\n",
       " 'banana-fish-2015-06-30-2016-02-27',\n",
       " 'banana-fish-2016-02-27-2016-10-26',\n",
       " 'banana-fish-2016-10-26-2017-06-25',\n",
       " 'banana-fish-2017-06-25-2018-02-22',\n",
       " 'banana-fish-2018-02-22-2018-10-22',\n",
       " 'banana-fish-2018-10-22-2019-06-22',\n",
       " 'bokuno-hero-academia-2006-03-21-2006-08-29',\n",
       " 'bokuno-hero-academia-2006-08-29-2007-02-06',\n",
       " 'bokuno-hero-academia-2007-02-06-2007-07-18',\n",
       " 'bokuno-hero-academia-2007-07-18-2007-12-26',\n",
       " 'bokuno-hero-academia-2007-12-26-2008-06-04',\n",
       " 'bokuno-hero-academia-2008-06-04-2008-11-13',\n",
       " 'bokuno-hero-academia-2008-11-13-2009-04-23',\n",
       " 'bokuno-hero-academia-2009-04-23-2009-10-01',\n",
       " 'bokuno-hero-academia-2009-10-01-2010-03-12',\n",
       " 'bokuno-hero-academia-2010-03-12-2010-08-20',\n",
       " 'bokuno-hero-academia-2010-08-20-2011-01-29',\n",
       " 'bokuno-hero-academia-2011-01-29-2011-07-09',\n",
       " 'bokuno-hero-academia-2011-07-09-2011-12-17',\n",
       " 'bokuno-hero-academia-2011-12-17-2012-05-27',\n",
       " 'bokuno-hero-academia-2012-05-27-2012-11-04',\n",
       " 'bokuno-hero-academia-2012-11-04-2013-04-14',\n",
       " 'bokuno-hero-academia-2013-04-14-2013-09-23',\n",
       " 'bokuno-hero-academia-2013-09-23-2014-03-03',\n",
       " 'bokuno-hero-academia-2014-03-03-2014-08-11',\n",
       " 'bokuno-hero-academia-2014-08-11-2015-01-20',\n",
       " 'bokuno-hero-academia-2015-01-20-2015-06-30']"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comic_daterange_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcloud compute  --project \"machinelearning-219614\"  instances create arb-tweet-crawler-danjon-meshi-2012-11-04-2013-07-04 arb-tweet-crawler-danjon-meshi-2013-07-04-2014-03-03 arb-tweet-crawler-danjon-meshi-2014-03-03-2014-10-31 arb-tweet-crawler-danjon-meshi-2014-10-31-2015-06-30 arb-tweet-crawler-danjon-meshi-2015-06-30-2016-02-27 arb-tweet-crawler-danjon-meshi-2016-02-27-2016-10-26 arb-tweet-crawler-danjon-meshi-2016-10-26-2017-06-25 arb-tweet-crawler-danjon-meshi-2017-06-25-2018-02-22 arb-tweet-crawler-danjon-meshi-2018-02-22-2018-10-22 arb-tweet-crawler-danjon-meshi-2018-10-22-2019-06-22 arb-tweet-crawler-banana-fish-2006-03-21-2006-11-18 arb-tweet-crawler-banana-fish-2006-11-18-2007-07-18 arb-tweet-crawler-banana-fish-2007-07-18-2008-03-16 arb-tweet-crawler-banana-fish-2008-03-16-2008-11-13 arb-tweet-crawler-banana-fish-2008-11-13-2009-07-13 arb-tweet-crawler-banana-fish-2009-07-13-2010-03-12 arb-tweet-crawler-banana-fish-2010-03-12-2010-11-09 arb-tweet-crawler-banana-fish-2010-11-09-2011-07-09 arb-tweet-crawler-banana-fish-2011-07-09-2012-03-07 arb-tweet-crawler-banana-fish-2012-03-07-2012-11-04 arb-tweet-crawler-banana-fish-2012-11-04-2013-07-04 arb-tweet-crawler-banana-fish-2013-07-04-2014-03-03 arb-tweet-crawler-banana-fish-2014-03-03-2014-10-31 arb-tweet-crawler-banana-fish-2014-10-31-2015-06-30 arb-tweet-crawler-banana-fish-2015-06-30-2016-02-27 arb-tweet-crawler-banana-fish-2016-02-27-2016-10-26 arb-tweet-crawler-banana-fish-2016-10-26-2017-06-25 arb-tweet-crawler-banana-fish-2017-06-25-2018-02-22 arb-tweet-crawler-banana-fish-2018-02-22-2018-10-22 arb-tweet-crawler-banana-fish-2018-10-22-2019-06-22 arb-tweet-crawler-bokuno-hero-academia-2006-03-21-2006-08-29 arb-tweet-crawler-bokuno-hero-academia-2006-08-29-2007-02-06 arb-tweet-crawler-bokuno-hero-academia-2007-02-06-2007-07-18 arb-tweet-crawler-bokuno-hero-academia-2007-07-18-2007-12-26 arb-tweet-crawler-bokuno-hero-academia-2007-12-26-2008-06-04 arb-tweet-crawler-bokuno-hero-academia-2008-06-04-2008-11-13 arb-tweet-crawler-bokuno-hero-academia-2008-11-13-2009-04-23 arb-tweet-crawler-bokuno-hero-academia-2009-04-23-2009-10-01 arb-tweet-crawler-bokuno-hero-academia-2009-10-01-2010-03-12 arb-tweet-crawler-bokuno-hero-academia-2010-03-12-2010-08-20 arb-tweet-crawler-bokuno-hero-academia-2010-08-20-2011-01-29 arb-tweet-crawler-bokuno-hero-academia-2011-01-29-2011-07-09 arb-tweet-crawler-bokuno-hero-academia-2011-07-09-2011-12-17 arb-tweet-crawler-bokuno-hero-academia-2011-12-17-2012-05-27 arb-tweet-crawler-bokuno-hero-academia-2012-05-27-2012-11-04 arb-tweet-crawler-bokuno-hero-academia-2012-11-04-2013-04-14 arb-tweet-crawler-bokuno-hero-academia-2013-04-14-2013-09-23 arb-tweet-crawler-bokuno-hero-academia-2013-09-23-2014-03-03 arb-tweet-crawler-bokuno-hero-academia-2014-03-03-2014-08-11 arb-tweet-crawler-bokuno-hero-academia-2014-08-11-2015-01-20 arb-tweet-crawler-bokuno-hero-academia-2015-01-20-2015-06-30 --machine-type \"g1-small\" --zone \"us-central1-a\" --image arb-tweet-crawler-base-3 --image-project \"machinelearning-219614\""
     ]
    }
   ],
   "source": [
    "save_path = './create_vm.sh'\n",
    "utils.to_shell(create_command, save_path, is_parallel=True)\n",
    "! cat $save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Delete Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stop_command(vm_index_list):\n",
    "    vm_name_list = [f'arb-tweet-crawler-{vm_index}' for vm_index in vm_index_list]\n",
    "    vm_names_text = ' '.join(vm_name_list)\n",
    "    command = f'gcloud compute  --project \"machinelearning-219614\"  instances stop {vm_names_text} --async --zone \"us-central1-a\"'\n",
    "    return command\n",
    "\n",
    "def make_delete_command(vm_index_list):\n",
    "    vm_name_list = [f'arb-tweet-crawler-{vm_index}' for vm_index in vm_index_list]\n",
    "    vm_names_text = ' '.join(vm_name_list)\n",
    "    command = f'gcloud compute  --project \"machinelearning-219614\"  instances delete {vm_names_text} --zone \"us-central1-a\"'\n",
    "    return command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "to_shell() missing 1 required positional argument: 'is_parallel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-330-379f8bf92b59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_stop_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvm_index_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mto_shell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./stop_vm.sh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: to_shell() missing 1 required positional argument: 'is_parallel'"
     ]
    }
   ],
   "source": [
    "c = make_stop_command(vm_index_list)\n",
    "to_shell(c, './stop_vm.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_index_list += [1, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = make_delete_command(vm_index_list)\n",
    "to_shell(c, './delete_vm.sh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcloud compute  --project \"machinelearning-219614\"  instances delete arb-tweet-crawler-1 arb-tweet-crawler-2 arb-tweet-crawler-3 arb-tweet-crawler-4 arb-tweet-crawler-5 arb-tweet-crawler-6 arb-tweet-crawler-7 arb-tweet-crawler-8 arb-tweet-crawler-9 arb-tweet-crawler-10 arb-tweet-crawler-11 arb-tweet-crawler-12 arb-tweet-crawler-13 arb-tweet-crawler-14 arb-tweet-crawler-15 arb-tweet-crawler-16 arb-tweet-crawler-17 arb-tweet-crawler-18 arb-tweet-crawler-19 arb-tweet-crawler-20 arb-tweet-crawler-21 arb-tweet-crawler-22 arb-tweet-crawler-23 arb-tweet-crawler-24 arb-tweet-crawler-25 arb-tweet-crawler-26 arb-tweet-crawler-27 arb-tweet-crawler-28 arb-tweet-crawler-29 arb-tweet-crawler-30 --zone \"us-central1-a\""
     ]
    }
   ],
   "source": [
    "# not_run_vm = [i for i in vm_index_list if i not in vm_crawl_index_list]\n",
    "c = make_delete_command(vm_index_list)\n",
    "to_shell(c, './delete_vm.sh')\n",
    "! cat ./delete_vm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run crawling at Instance\n",
    "ローカルからgce内のdockerへ実行コマンドを送るやつ\n",
    "テストで実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# container_command = f\"python main.py {query}  --since {since} --until {until}\"\n",
    "# docker_command = f\"docker run -d -v --rm /home/syunyooo/auto-ranking-blog/crawler/tweet-crawler/:/app tweet-crawler_jupyter {container_command}\"\n",
    "# docker_command = f\"git -C /home/syunyooo/auto-ranking-blog/ pull\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_command(query, vm_index, since='2006-03-21', until='2019-06-12'):\n",
    "    user_name = 'syunyooo'\n",
    "    vm_name = f'arb-tweet-crawler-{vm_index}'\n",
    "    vm_command = f'sh /home/syunyooo/auto-ranking-blog/crawler/tweet-crawler/scripts/run-docker-crawl-shutdown.sh {query}  --since {since} --until {until}'\n",
    "    local_command = f'gcloud compute --project \"machinelearning-219614\" ssh --zone \"us-central1-a\" \"{user_name}@{vm_name}\" --command \"{vm_command}\"'\n",
    "    return local_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cmd_list = [make_command(c['query'], c['name'], c['since'], c['until']) for c in _crawl_query_instance_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(run_cmd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './run_crawl.sh'\n",
    "to_shell(run_cmd_list, save_path, is_parallel=True)\n",
    "! cat $save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hall_of_frame_serialized': ['ワンピース', 'キングダム', '名探偵コナン', 'ジョジョ', 'キン肉マン'],\n",
       " 'hall_of_frame_ended': ['ナルト', 'ドラゴンボール', 'キャプテン翼', 'ガッシュ', '銀魂'],\n",
       " 'famous_from_before': ['僕のヒーローアカデミア', '亜人', '進撃の巨人', 'テラフォーマー', 'ワンパンマン'],\n",
       " 'famous_recently': ['約束のネバーランド', 'Dr.STONE', 'チェンソーマン', 'かぐや様', 'ダンジョン飯'],\n",
       " 'geek': ['ワンナウツ', 'BANANA FISH', 'AKIRA', '君に届け', '狼陛下の花嫁'],\n",
       " 'obscure': ['フィンランドサガ',\n",
       "  '雀荘のサエコさん',\n",
       "  'ジモトがジャパン',\n",
       "  'SILENT NIGHT 翔',\n",
       "  '私立ポセイドン学園高等部',\n",
       "  'チャゲチャ',\n",
       "  '大泥棒ポルタ']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "comic_list = []\n",
    "for k,v in comic_dict.items():\n",
    "    comic_list += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 処理済み\n",
    "ignore_comic_list = ['ワンピース', '進撃の巨人', 'チェンソーマン', 'ジョジョ', 'ジモトがジャパン',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "comic_list = [comic for comic in comic_list if comic not in ignore_comic_list]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
